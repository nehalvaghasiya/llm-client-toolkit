# LLMÂ ClientÂ Toolkit

## Table of Contents

* [Overview](#overview)
* [Technical Architecture](#technical-architecture)
* [Installation](#installation)

  * [Using **uv**Â (recommended)](#using-uv-recommended)
  * [Using `pip`Â /Â virtualâ€‘env](#using-pip--virtualenv)
* [QuickÂ Start](#quick-start)
* [Troubleshooting](#troubleshooting)
* [DirectoryÂ Tree](#directory-tree)
* [ContributingÂ â€”Â BugsÂ &Â FeatureÂ Requests](#contributing--bugs--feature-requests)
* [TechnologiesÂ Used](#technologies-used)


## Overview

`llm_client` is a **frameworkâ€‘agnostic**, easily extensible toolkit that wraps any Largeâ€‘Languageâ€‘Model backendâ€”OpenAI, Anthropic, local models, etc.â€”behind a single, clean interface.

It is designed for developers who want:

* **Sync & async** support outâ€‘ofâ€‘theâ€‘box.
* **Parserâ€‘driven retries** that autoâ€‘repair malformed model output.
* Dropâ€‘in **adapters** for LangChain, Llamaâ€‘Index, Pydanticâ€‘AI and friends.
* A **Pydanticâ€‘powered configuration** system you can load from `config.yaml`, environment variables, or code.

> **Why do I need this?**  If you have ever peppered your codebase with adâ€‘hoc OpenAI calls, copyâ€‘pasted retry loops, or fought with schema validation, `llm_client` gives you a reusable core so you can focus on *product* rather than plumbing.


## Technical Architecture

| Layer               | Responsibility                                                                            | KeyÂ Files                      |
| ------------------- | ----------------------------------------------------------------------------------------- | ------------------------------ |
| **Configuration**   | Stronglyâ€‘typed values loaded from YAML /Â env                                              | `llm_client/config.py`         |
| **CoreÂ Client**     | Abstract `LLMClient` with sync `generate()` and async `agenerate()`                       | `llm_client/base.py`           |
| **Backâ€‘ends**       | Concrete clients (e.g. `OpenAILLMClient`) that implement a single `_acomplete()` method   | `llm_client/openai_backend.py` |
| **Parsing**         | Strategy objects that turn raw text â†’ structured data (`JsonParser`, `PydanticParser`, â€¦) | `llm_client/parsing.py`        |
| **RetryÂ Decorator** | Wraps any client with parserâ€‘aware retries                                                | `llm_client/retry.py`          |
| **Adapters**        | Optional helpers that convert an `LLMClient` into LangChain/LlamaIndex objects            | `llm_client/adapters.py`       |


## Installation

`llm_client` works with **PythonÂ 3.10 â€“Â 3.13**.

### Using **uv** (recommended)

[`uv`](https://docs.astral.sh/uv/getting-started/features/#projects) is a superâ€‘fast Rust replacement for `pip` + `virtualenv`. It creates deterministic lock files and dramatically speeds up installs.

```bash
# 1Â â€”Â InstallÂ uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2Â â€”Â Create & activate a virtualenv (optional)
uv venv .venv
source .venv/bin/activate 

# 3Â â€”Â Lock and sync project dependencies
uv lock                          # generates uv.lock if missing
uv sync                          # installs deps exactly as in uv.lock
```

### Using `pip` /Â virtualâ€‘env

```bash
python -m venv .venv
source .venv/bin/activate       # Windows: .venv\Scripts\activate
pip install -r requirements.txt # will be generated from uv.lock if you prefer pip
```

> **Note**Â Â If you plan to use OpenAI or Grok, remember to export your key:
>
> ```bash
> export OPENAI_API_KEY="skâ€‘..."
> ```

---

## QuickÂ Start

```python
from llm_client import (
    LLMConfig,
    OpenAILLMClient,
    RetryLLMClient,
    JsonParser,
)

cfg    = LLMConfig.from_yaml("config.yaml")
base   = OpenAILLMClient(cfg)
client = RetryLLMClient(base, JsonParser())

answer = client.generate("Return a JSON greet: {\"greeting\": \"...\"}")
print(answer)
```

Async usage:

```python
import asyncio
from llm_client import OpenAILLMClient, LLMConfig

async def main():
    cfg = LLMConfig.from_env()
    async with OpenAILLMClient(cfg) as llm:
        print(await llm.agenerate("Hi! Who are you?"))

asyncio.run(main())
```

Need LangChain? Just adapt:

```python
from llm_client import OpenAILLMClient, LLMConfig
from llm_client.adapters import to_langchain

lc_chat = to_langchain(OpenAILLMClient(LLMConfig.from_env()))
print(lc_chat.invoke("Summarise this repo in two bullets"))
```

---

## Configuration (`config.yaml`) &Â EnvironmentÂ Variables

`config.yaml` is the single place to tweak **model**, **generation parameters**, and **retry behaviour**.  Typical keys:

```yaml
# Model & generation
model: "gpt-4o-mini"
temperature: 0.7
max_tokens: 1024

# Retry wrapper
retry_count: 3

# Transport â€”Â `api_key` is OPTIONAL here
# Prefer loading secrets from the environment for safety
api_key: ""   # leave blank or delete this key
```

> **Why envÂ vars for secrets?**
> Hardâ€‘coding your OpenAI key (or any credential) into a repo is a security risk. By keeping `api_key` empty the client will automatically fall back to the `OPENAI_API_KEY` environment variable.

### Exporting the key

```bash
# macOS/Linux
export OPENAI_API_KEY="skâ€‘â€¦"

# Windows PowerShell
setx OPENAI_API_KEY "skâ€‘â€¦"
```

### Example usage in code

```python
from llm_client import LLMConfig, OpenAILLMClient

cfg = LLMConfig.from_yaml("config.yaml")  # `api_key` is resolved from env
client = OpenAILLMClient(cfg)
print(client.generate("Say hi as JSON {\"greeting\":\"...\"}"))
```

---

## Troubleshooting

| Symptom                         | Fix                                                                                                                |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| *`ModuleNotFoundError: openai`* | `pip install openai` or `uv pip install openai`                                                                    |
| Parser keeps failing            | Increase `retry_count` in `config.yaml` or improve your prompt so the output actually matches the expected schema. |
| Timeouts on large payloads      | Bump `timeout` in `config.yaml` or stream tokens (planned feature).                                                |

If you hit something gnarlier, feel free to open an [issue](https://github.com/nehalvaghasiya/llm-client-toolkit/issues).



## Optional Adapters (LangChain,Â LlamaIndex, Pydanticâ€‘AI,Â â€¦)

The **core package** has *no hard dependency* on external framework SDKs. If you want to use the convenience wrappers in `llm_client.adapters`, install the matching library yourself:

| Adapter helper     | Extra dependency                         | Install command                |
| ------------------ | ---------------------------------------- | ------------------------------ |
| `to_langchain()`   | `langchain`Â (or `langchain-openai`)      | `pip install langchain-openai` |
| `to_llamaindex()`  | `llama-index`Â >=Â 0.10                    | `pip install llama-index`      |
| `to_pydantic_ai()` | `pydantic-ai` (hypothetical placeholder) | `pip install "pydantic-ai-slim[openai]"`      |

If an adapter is invoked without the required package present, an `ImportError` will be raised with guidance on what to install.


## Directory Tree

```text
llm_project/
â”œâ”€â”€ llm_client/                # Reusable Python package
â”‚   â”œâ”€â”€ __init__.py            # Public reâ€‘exports
â”‚   â”œâ”€â”€ base.py                # Abstract client + helpers
â”‚   â”œâ”€â”€ config.py              # Pydantic model & YAML loader
â”‚   â”œâ”€â”€ parsing.py             # Parsers + ParserError
â”‚   â”œâ”€â”€ openai_backend.py      # Async OpenAI implementation
â”‚   â”œâ”€â”€ retry.py               # Parserâ€‘aware retry decorator
â”‚   â””â”€â”€ adapters.py            # LangChain /Â LlamaIndex utilities
â”œâ”€â”€ examples/                  # Readyâ€‘toâ€‘run demos
â”‚   â”œâ”€â”€ demo_sync.py
â”‚   â””â”€â”€ demo_async.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml            # Default settings (copy & edit)
â”œâ”€â”€ uv.lock                    # Exact dependency versions (autoâ€‘generated)
â”œâ”€â”€ requirements.txt           # Generated from uv.lock for pip users
â””â”€â”€ README.md                  # You are here ðŸ“–
```

## ContributingÂ â€”Â BugsÂ &Â FeatureÂ Requests

Spotted a bug, need another backend, or fancy streamingâ€‘token support?

* **Bug?**Â Â [Open an issue](https://github.com/nehalvaghasiya/llm-client-toolkit/issues) with a minimal, reproducible example.
* **Feature?**Â Â Describe the useâ€‘case; PRs are welcome once we agree on the scope.

